import duckdb
import os
import json
import pandas as pd
from sentence_transformers import SentenceTransformer

# ==========================================
# 1. ì„¤ì • (íŒŒì¼ ê²½ë¡œ ë° ëª¨ë¸)
# ==========================================
INPUT_FILE = 'optimized_farming_data_v2.jsonl'  # ì›ë³¸ ë°ì´í„°
DB_PATH = 'farming_granular.duckdb'             # ìƒì„±ë  DB ì´ë¦„
MODEL_NAME = 'jhgan/ko-sroberta-multitask'      # í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸
VECTOR_DIM = 768                                # ëª¨ë¸ì˜ ë²¡í„° ì°¨ì› ìˆ˜

# ==========================================
# 2. AI ëª¨ë¸ ë¡œë“œ
# ==========================================
print(f"ğŸš€ [1/5] AI ëª¨ë¸ ë¡œë“œ ì¤‘ ({MODEL_NAME})...")
model = SentenceTransformer(MODEL_NAME)

# ==========================================
# 3. DuckDB ì´ˆê¸°í™” ë° í…Œì´ë¸” ìƒì„±
# ==========================================
print(f"ğŸš€ [2/5] ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘...")

# ê¸°ì¡´ DB íŒŒì¼ì´ ìˆë‹¤ë©´ ì‚­ì œ (ê¹¨ë—í•œ ìƒíƒœë¡œ ì‹œì‘)
if os.path.exists(DB_PATH):
    try:
        os.remove(DB_PATH)
    except PermissionError:
        print("âŒ ì˜¤ë¥˜: DB íŒŒì¼ì´ ì—´ë ¤ìˆì–´ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. DB ì—°ê²°ì„ í•´ì œí•´ì£¼ì„¸ìš”.")
        exit()

con = duckdb.connect(DB_PATH)

# ë²¡í„° ê²€ìƒ‰ í™•ì¥ ê¸°ëŠ¥(VSS) ë¡œë“œ
try:
    con.execute("INSTALL vss; LOAD vss;")
except Exception as e:
    print(f"âš ï¸ í™•ì¥ ë¡œë“œ ê²½ê³  (ì´ë¯¸ ì„¤ì¹˜ëœ ê²½ìš° ë¬´ì‹œ): {e}")

# í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì •ì˜ (ì¹´í…Œê³ ë¦¬ë³„ë¡œ ìª¼ê°œì§„ êµ¬ì¡°)
con.execute(f"""
    CREATE TABLE farming (
        id TEXT,
        year TEXT,
        month INTEGER,
        category TEXT,     -- 'ì–‘ë´‰', 'ê¸°ìƒ', 'ë²¼' ë“± êµ¬ë¶„
        content TEXT,      -- ì‹¤ì œ ë‚´ìš© (ê¸°í˜¸, íŠ¹ìˆ˜ë¬¸ì ë³´ì¡´ë¨)
        embedding FLOAT[{VECTOR_DIM}]
    )
""")

# ==========================================
# 4. ë°ì´í„° ì½ê¸° ë° ì „ì²˜ë¦¬ (Flattening)
# ==========================================
print(f"ğŸš€ [3/5] JSONL íŒŒì¼ ì½ê¸° ë° ë°ì´í„° ì„¸ë¶„í™”...")

processed_rows = []
texts_to_embed = []

try:
    with open(INPUT_FILE, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f):
            if not line.strip(): continue
            
            entry = json.loads(line)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            week_id = entry.get('id')
            year = entry.get('year')
            month = entry.get('month')
            
            # ë°ì´í„° êµ¬ì¡° íŒŒì•… (í‰íƒ„í™” ì—¬ë¶€ì— ë”°ë¼ ì²˜ë¦¬)
            # 1) {"content": {"ì–‘ë´‰": "..."}} í˜•íƒœì¸ ê²½ìš°
            if 'content' in entry and isinstance(entry['content'], dict):
                target_dict = entry['content']
            # 2) {"ì–‘ë´‰": "...", "ê¸°ìƒ": "..."} í˜•íƒœì¸ ê²½ìš° (ì´ë¯¸ í‰íƒ„í™”ë¨)
            else:
                target_dict = entry

            # ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë°ì´í„° ë¶„ë¦¬ (Granular Split)
            for key, val in target_dict.items():
                # ë©”íƒ€ë°ì´í„° í‚¤ëŠ” ê±´ë„ˆëœ€
                if key in ['id', 'year', 'month', 'week_range', 'start_date', 'end_date']:
                    continue
                
                # ìœ íš¨í•œ ë°ì´í„°ì¸ì§€ í™•ì¸ (ë„ˆë¬´ ì§§ê±°ë‚˜ ë¹„ì–´ìˆìœ¼ë©´ ì œì™¸)
                if not val or not isinstance(val, str) or len(val.strip()) < 5:
                    continue

                # ì›ë³¸ í…ìŠ¤íŠ¸ ê³µë°±ë§Œ ì •ë¦¬ (%, ~, â„ƒ ë“± íŠ¹ìˆ˜ê¸°í˜¸ ì™„ë²½ ë³´ì¡´)
                clean_content = val.strip()
                
                # ì„ë² ë”© í’ˆì§ˆì„ ìœ„í•´ "ì¹´í…Œê³ ë¦¬: ë‚´ìš©" í˜•íƒœë¡œ ì¡°í•©
                # ì˜ˆ: "ì–‘ë´‰: ê²¨ìš¸ì²  ì˜¨ë„ëŠ” -2~5â„ƒ ìœ ì§€..."
                embedding_text = f"{key}: {clean_content}"
                
                # ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                processed_rows.append({
                    "id": week_id,
                    "year": year,
                    "month": month,
                    "category": key,
                    "content": clean_content
                })
                texts_to_embed.append(embedding_text)

except FileNotFoundError:
    print(f"âŒ ì˜¤ë¥˜: ì…ë ¥ íŒŒì¼({INPUT_FILE})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    exit()

print(f"   -> ì´ {len(processed_rows)}ê°œì˜ ì„¸ë¶€ ë°ì´í„°ë¡œ ë¶„ë¦¬ ì™„ë£Œ.")

# ==========================================
# 5. ì„ë² ë”© ìƒì„± ë° DB ì €ì¥ (Pandas ê³ ì† ëª¨ë“œ)
# ==========================================
if texts_to_embed:
    print(f"ğŸš€ [4/5] ì„ë² ë”© ìƒì„± ë° ê³ ì† ì €ì¥ ì‹œì‘ ({len(texts_to_embed)}ê±´)...")
    
    # 1) ì„ë² ë”© ìƒì„± (Batch Processing)
    vectors = model.encode(texts_to_embed, batch_size=64, show_progress_bar=True, normalize_embeddings=True)
    
    # 2) Pandas DataFrame ìƒì„± (ë³‘ëª© í•´ê²°ì˜ í•µì‹¬)
    df = pd.DataFrame(processed_rows)
    df['embedding'] = list(vectors) # ë²¡í„° ì»¬ëŸ¼ ì¶”ê°€
    
    # 3) DuckDBì— í†µì§¸ë¡œ ì…ë ¥ (SQL Injection ë°©ì§€ ë° ì†ë„ ìµœì í™”)
    # dfì˜ ì»¬ëŸ¼ ìˆœì„œê°€ í…Œì´ë¸”ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ëª…ì‹œì ìœ¼ë¡œ ë§¤í•‘í•˜ê±°ë‚˜ ìˆœì„œë¥¼ ë§ì¶¤
    # ì—¬ê¸°ì„œëŠ” DataFrame í‚¤ ìˆœì„œì™€ í…Œì´ë¸” ì •ì˜ê°€ ê±°ì˜ ê°™ìœ¼ë¯€ë¡œ ë°”ë¡œ ì‚½ì… ì‹œë„
    # ì•ˆì „í•˜ê²Œ ì»¬ëŸ¼ ìˆœì„œ ì¬ë°°ì—´:
    df = df[['id', 'year', 'month', 'category', 'content', 'embedding']]
    
    print("   -> DBì— ë°ì´í„° ì…ë ¥ ì¤‘ (Bulk Insert)...")
    con.execute("INSERT INTO farming SELECT * FROM df")
    
    # 4) ì¸ë±ìŠ¤ ìƒì„±
    print("ğŸš€ [5/5] ë²¡í„° ê²€ìƒ‰ ì¸ë±ìŠ¤(HNSW) ìƒì„± ì¤‘...")
    try:
        con.execute("SET hnsw_enable_experimental_persistence = true;")
        con.execute("CREATE INDEX idx_vector ON farming USING HNSW (embedding)")
    except Exception as e:
        print(f"âš ï¸ ì¸ë±ìŠ¤ ìƒì„± ê²½ê³  (ë°ì´í„°ëŠ” ì •ìƒ ì €ì¥ë¨): {e}")

else:
    print("âš ï¸ ì²˜ë¦¬í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ==========================================
# 6. ë§ˆë¬´ë¦¬
# ==========================================
con.close()
print("="*50)
print(f"âœ… ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
print(f"ğŸ“‚ ìƒì„±ëœ íŒŒì¼: {os.path.abspath(DB_PATH)}")
print("="*50)